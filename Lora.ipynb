{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LSNAu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, GPTQConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"beomi/Llama-3-Open-Ko-8B-Instruct-preview\" #Llama3모델을 이용해서 학습합니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             return_dict=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map='auto',\n",
    "                                             load_in_8bit=True\n",
    "                                            )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('preprocessing_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_result = df.apply(lambda row: {\"input\": row['Prev_Message'], \"output\": row['My_Response']}, axis=1).tolist() #json형식으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON으로 저장\n",
    "with open('dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_result, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open('dataset.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 모델 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\")\n",
    "\n",
    "# 최대 시퀀스 길이 설정\n",
    "max_sequence_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_chat_completion(dialog, system_token=\"### System:\", user_token=\"### Instruction:\", assistant_token=\"### Response:\", start_token=\"\", end_token=\"\"):\n",
    "    role_dict = {\"input\": user_token, \"output\": assistant_token}\n",
    "    \n",
    "    def format_exchange(exchange):\n",
    "        return (\n",
    "            f\"{role_dict['input']}\\n\"\n",
    "            f\"{start_token}{exchange['input'].strip()}{end_token}\\n\\n\"\n",
    "            f\"{role_dict['output']}\\n\"\n",
    "            f\"{start_token}{exchange['output'].strip()}{end_token}</s>\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    dialog_text = [format_exchange(exchange) for exchange in dialog]\n",
    "    dialog_tokens = ''.join(dialog_text).replace('\\n\\n</s>\\n\\n', '</s>\\n\\n')\n",
    "    \n",
    "    return f'<s>{dialog_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [prompt_chat_completion([item]) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>### Instruction:\\n나 군대 선임중에 유위웅이라는 사람 있었음 유위웅~~ 위웅 위웅~\\n\\n### Response:\\n위웅</s>\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomDataset 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_sequence_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]\n",
    "        \n",
    "        prompt_templete = conversation\n",
    "    \n",
    "        # 텍스트를 토큰화하고 인코딩\n",
    "        encoding = self.tokenizer(\n",
    "            prompt_templete,\n",
    "            truncation=True,\n",
    "            max_length=self.max_sequence_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(split_data, tokenizer, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token #패딩토큰 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=custom_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"./output\",\n",
    "        optim='adamw_8bit'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False \n",
    "trainer.train()\n",
    "trainer.save_model(\"./lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# 중지 기준이 되는 토큰들이 생성된 텍스트에 포함되는지 확인하는 클래스\n",
    "class _SentinelTokenStoppingCriteria(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, sentinel_token_ids: torch.LongTensor, starting_idx: int):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids  # 중지 기준 토큰 ID\n",
    "        self.starting_idx = starting_idx  # 시작 인덱스\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, _scores: torch.FloatTensor) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            if trimmed_sample.shape[-1] < self.sentinel_token_ids.shape[-1]:\n",
    "                continue\n",
    "            for window in trimmed_sample.unfold(0, self.sentinel_token_ids.shape[-1], 1):\n",
    "                if torch.all(torch.eq(self.sentinel_token_ids, window)):\n",
    "                    return True  # 중지 기준 토큰 발견 시 True 반환\n",
    "        return False\n",
    "\n",
    "# 여러 중지 조건을 받아서 StoppingCriteriaList 반환\n",
    "def stopping_criteria_list(stopping_list:list, tokenizer):\n",
    "    stop_tokens = []\n",
    "    for i in stopping_list:\n",
    "        token = tokenizer(i, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        stop_token = token.input_ids.cuda()\n",
    "        stop_tokens.append(stop_token)\n",
    "\n",
    "    sentinel_tokens = []\n",
    "    for j in stop_tokens:\n",
    "        sentinel_tokens.append(_SentinelTokenStoppingCriteria(\n",
    "            sentinel_token_ids=j, starting_idx=token.input_ids.shape[-1]\n",
    "        ))\n",
    "\n",
    "    return StoppingCriteriaList(sentinel_tokens)\n",
    "\n",
    "# 중지 조건 리스트 정의 및 생성\n",
    "early_stopping_list = stopping_criteria_list([\"####\",\"Instruct\", \"Instruction\",\"\\n#\",'</s>'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### Instruction:\n",
      "뭐 하고 싶어?\n",
      "\n",
      "### Response: \n",
      "게임\n",
      "\n",
      "Instruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  14711,  30151,    512,    167,  99834, 107973, 107719,  32179,\n",
       "           1980,  14711,   6075,     25,    720, 111807,    271,  17077]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "p = \"뭐 하고 싶어?\"\n",
    "input_ids = tokenizer(f\"### Instruction:\\n{p}\\n\\n### Response:\", max_length=128, truncation=True, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids,\n",
    "                        streamer=TextStreamer(tokenizer),\n",
    "                        stopping_criteria=early_stopping_list,\n",
    "                        temperature=0.7,\n",
    "                        repetition_penalty=1.5,\n",
    "                        max_new_tokens=128,\n",
    "                       )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### Instruction:\n",
      "너가 제일 좋아하는 음식은 뭐야?\n",
      "\n",
      "### Response: \n",
      "아무래도 피자\n",
      "\n",
      "Instruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  14711,  30151,    512, 105078,  20565,  63171,  33177, 117004,\n",
       "          44005, 106318,  77437,  34804, 113792,  90759,   1980,  14711,   6075,\n",
       "             25,    720,  54059, 100981,  54542,  49085, 104064,  26799,    271,\n",
       "          17077]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"너가 제일 좋아하는 음식은 뭐야?\"\n",
    "input_ids = tokenizer(f\"### Instruction:\\n{p}\\n\\n### Response:\", max_length=128, truncation=True, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids,\n",
    "                        streamer=TextStreamer(tokenizer),\n",
    "                        stopping_criteria=early_stopping_list,\n",
    "                        temperature=0.7,\n",
    "                        repetition_penalty=1.5,\n",
    "                        max_new_tokens=128,\n",
    "                       )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LSNAu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### Instruction:\n",
      "너가 제일 과목은 뭐야?\n",
      "\n",
      "### "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LSNAu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LSNAu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\LSNAu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "과학이랑 수리\n",
      "\n",
      "Instruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  14711,  30151,    512, 105078,  20565,  63171,  33177, 104219,\n",
       "          88708,  34804, 113792,  90759,   1980,  14711,   6075,     25,    720,\n",
       "          54780, 100508,  13094, 102581,  29833,  29102,    271,  17077]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"너가 제일 과목은 뭐야?\"\n",
    "input_ids = tokenizer(f\"### Instruction:\\n{p}\\n\\n### Response:\", max_length=128, truncation=True, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids,\n",
    "                        streamer=TextStreamer(tokenizer),\n",
    "                        stopping_criteria=early_stopping_list,\n",
    "                        temperature=0.7,\n",
    "                        repetition_penalty=1.5,\n",
    "                        max_new_tokens=128,\n",
    "                       )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### Instruction:\n",
      "해외여행 간다면 어디?\n",
      "\n",
      "### Response: \n",
      "일본\n",
      "\n",
      "Instruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  14711,  30151,    512,  34983, 104065,  58126, 101066, 105131,\n",
       "         115300, 117337,   1980,  14711,   6075,     25,    720, 123256,    271,\n",
       "          17077]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"해외여행 간다면 어디?\"\n",
    "input_ids = tokenizer(f\"### Instruction:\\n{p}\\n\\n### Response:\", max_length=128, truncation=True, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids,\n",
    "                        streamer=TextStreamer(tokenizer),\n",
    "                        stopping_criteria=early_stopping_list,\n",
    "                        temperature=0.7,\n",
    "                        repetition_penalty=1.5,\n",
    "                        max_new_tokens=128,\n",
    "                       )\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
